version: '3.8'

networks:
  spark-network:

x-defaults:
  &spark-common
  image: bitnami/spark:3.4.0
  user: root
  environment:
    - SPARK_CONF_DIR=/opt/bitnami/spark/conf
    - SPARK_EVENTLOG_ENABLED=true
    - SPARK_EVENTLOG_DIR=/opt/bitnami/spark/logs
    - SPARK_HISTORY_OPTS=-Dspark.history.fs.logDirectory=file:/opt/bitnami/spark/logs
    - SPARK_SQL_EXTENSIONS=io.delta.sql.DeltaSparkSessionExtension
    - SPARK_SQL_CATALOG_IMPLEMENTATION=io.delta.sql.DeltaCatalog
  networks:
    - spark-network
  volumes:
    - ./lakehouse:/opt/bitnami/spark/data
    - ./logs/spark:/opt/bitnami/spark/logs

services:
  spark-master:
    <<: *spark-common
    container_name: spark-master
    hostname: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8080
    ports:
      - 8080:8080
      - 7077:7077

  spark-worker:
    <<: *spark-common
    container_name: spark-worker
    hostname: spark-worker
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_DRIVER_MEMORY=4g
      - SPARK_EXECUTOR_MEMORY=4g
      - SPARK_DRIVER_CORES=3
      - SPARK_EXECUTOR_CORES=10
    ports:
      - 4040:4040

  spark-history-server:
    <<: *spark-common
    container_name: spark-history-server
    hostname: spark-history-server
    depends_on:
      - spark-master
    ports:
      - 18080:18080
    command: >
      bash -c "/opt/bitnami/spark/sbin/start-history-server.sh"

  spark-driver:
    image: spark-driver
    container_name: spark-driver
    hostname: spark-driver
    user: appuser
    tty: true
    depends_on:
      - spark-worker 
    build:
      context: .
      dockerfile: ./Dockerfile
    networks:
      - spark-network
    ports:
      - 8888:8888
    volumes:
      - .:/home/appuser/code/ufo-lakehouse
